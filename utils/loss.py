# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Loss functions
"""

import torch
import torch.nn as nn

from utils.metrics import bbox_iou, wasserstein_loss, box_iou
from utils.torch_utils import de_parallel
from utils.assigner_utils import *
from utils.general import xywh2xyxy

def smooth_BCE(eps=0.1):  # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441
    # return positive, negative label smoothing BCE targets
    return 1.0 - 0.5 * eps, 0.5 * eps


class BCEBlurWithLogitsLoss(nn.Module):
    # BCEwithLogitLoss() with reduced missing label effects.
    def __init__(self, alpha=0.05):
        super().__init__()
        self.loss_fcn = nn.BCEWithLogitsLoss(reduction='none')  # must be nn.BCEWithLogitsLoss()
        self.alpha = alpha

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)
        pred = torch.sigmoid(pred)  # prob from logits
        dx = pred - true  # reduce only missing label effects
        # dx = (pred - true).abs()  # reduce missing label and false label effects
        alpha_factor = 1 - torch.exp((dx - 1) / (self.alpha + 1e-4))
        loss *= alpha_factor
        return loss.mean()


class FocalLoss(nn.Module):
    # Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)
    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):
        super().__init__()
        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = loss_fcn.reduction
        self.loss_fcn.reduction = 'none'  # required to apply FL to each element

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)
        # p_t = torch.exp(-loss)
        # loss *= self.alpha * (1.000001 - p_t) ** self.gamma  # non-zero power for gradient stability

        # TF implementation https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py
        pred_prob = torch.sigmoid(pred)  # prob from logits
        p_t = true * pred_prob + (1 - true) * (1 - pred_prob)
        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)
        modulating_factor = (1.0 - p_t) ** self.gamma
        loss *= alpha_factor * modulating_factor

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss


class QFocalLoss(nn.Module):
    # Wraps Quality focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)
    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):
        super().__init__()
        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = loss_fcn.reduction
        self.loss_fcn.reduction = 'none'  # required to apply FL to each element

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)

        pred_prob = torch.sigmoid(pred)  # prob from logits
        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)
        modulating_factor = torch.abs(true - pred_prob) ** self.gamma
        loss *= alpha_factor * modulating_factor

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss


class ComputeLoss:
    sort_obj_iou = False

    # Compute losses
    def __init__(self, model, NWD_ratio=0, autobalance=False):
        device = next(model.parameters()).device  # get model device
        h = model.hyp  # hyperparameters
        self.NWD_ratio = NWD_ratio

        # Define criteria
        BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['cls_pw']], device=device))
        BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['obj_pw']], device=device))

        # Class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3
        self.cp, self.cn = smooth_BCE(eps=h.get('label_smoothing', 0.0))  # positive, negative BCE targets

        # Focal loss
        g = h['fl_gamma']  # focal loss gamma
        if g > 0:
            BCEcls, BCEobj = FocalLoss(BCEcls, g, 0.5), FocalLoss(BCEobj, g, 0.5)

        #   è·å–åˆ°æ¨¡å‹çš„Detectæ¨¡å—
        m = de_parallel(model).model[-1]  # Detect() module
        self.balance = {3: [4.0, 1.0, 0.4]}.get(m.nl, [4.0, 1.0, 0.25, 0.06, 0.02])  # P3-P7
        self.ssi = list(m.stride).index(16) if autobalance else 0  # stride 16 index
        self.BCEcls, self.BCEobj, self.gr, self.hyp, self.autobalance = BCEcls, BCEobj, 1.0, h, autobalance
        self.na = m.na  # number of anchors
        self.nc = m.nc  # number of classes
        self.nl = m.nl  # number of layers
        self.anchors = m.anchors
        self.device = device

    def __call__(self, p, targets):  # predictions, targets
        #   pä¸ºæ¨¡å‹çš„é¢„æµ‹è¾“å‡º[(bs, 3, 80, 80, 85),(bs, 3, 40, 40, 85),(bs, 3, 20, 20, 85)]
        #   targetsä¸ºæ ‡ç­¾ä¿¡æ¯ targetæ˜¯shapeä¸ºï¼ˆç›®æ ‡æ•°ï¼Œ6ï¼‰çš„tensorï¼Œç¬¬ä¸€åˆ—è¡¨ç¤ºæ ‡ç­¾æ‰€å±çš„å›¾ç‰‡æ ‡ç­¾0~63
        lcls = torch.zeros(1, device=self.device)  # class loss
        lbox = torch.zeros(1, device=self.device)  # box loss
        lobj = torch.zeros(1, device=self.device)  # object loss
        # æ¯ä¸€ä¸ªéƒ½æ˜¯appendçš„ æœ‰feature mapä¸ª æ¯ä¸ªéƒ½æ˜¯å½“å‰è¿™ä¸ªfeature mapä¸­3ä¸ªanchorç­›é€‰å‡ºçš„æ‰€æœ‰çš„target(3ä¸ªgrid_cellè¿›è¡Œé¢„æµ‹)
        # tcls: è¡¨ç¤ºè¿™ä¸ªtargetæ‰€å±çš„class index
        # tbox: xywh å…¶ä¸­xyä¸ºè¿™ä¸ªtargetå¯¹å½“å‰grid_cellå·¦ä¸Šè§’çš„åç§»é‡
        # indices: b: è¡¨ç¤ºè¿™ä¸ªtargetå±äºçš„image index
        #          a: è¡¨ç¤ºè¿™ä¸ªtargetä½¿ç”¨çš„anchor index
        #          gj: ç»è¿‡ç­›é€‰åç¡®å®šæŸä¸ªtargetåœ¨æŸä¸ªç½‘æ ¼ä¸­è¿›è¡Œé¢„æµ‹(è®¡ç®—æŸå¤±)  gjè¡¨ç¤ºè¿™ä¸ªç½‘æ ¼çš„å·¦ä¸Šè§’yåæ ‡
        #          gi: è¡¨ç¤ºè¿™ä¸ªç½‘æ ¼çš„å·¦ä¸Šè§’xåæ ‡
        # anch: è¡¨ç¤ºè¿™ä¸ªtargetæ‰€ä½¿ç”¨anchorçš„å°ºåº¦ï¼ˆç›¸å¯¹äºè¿™ä¸ªfeature mapï¼‰  æ³¨æ„å¯èƒ½ä¸€ä¸ªtargetä¼šä½¿ç”¨å¤§å°ä¸åŒanchorè¿›è¡Œè®¡ç®—
        tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets

        # Losses
        for i, pi in enumerate(p):  # layer index, layer predictions
            b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx
            tobj = torch.zeros(pi.shape[:4], dtype=pi.dtype, device=self.device)  # target obj

            n = b.shape[0]  # number of targets
            if n:
                # pxy, pwh, _, pcls = pi[b, a, gj, gi].tensor_split((2, 4, 5), dim=1)  # faster, requires torch 1.8.0
                pxy, pwh, _, pcls = pi[b, a, gj, gi].split((2, 2, 1, self.nc), 1)  # target-subset of predictions

                # Regression
                pxy = pxy.sigmoid() * 2 - 0.5
                pwh = (pwh.sigmoid() * 2) ** 2 * anchors[i]
                pbox = torch.cat((pxy, pwh), 1)  # predicted box
                iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze()  # iou(prediction, target)

                if self.NWD_ratio != 0:
                    nwd_ratio = 0.5  # å¹³è¡¡nwdå’ŒåŸå§‹iouçš„æƒé‡
                    nwd = wasserstein_loss(pbox, tbox[i]).squeeze()
                    lbox += (1 - nwd_ratio) * (1.0 - nwd).mean() + nwd_ratio * (1.0 - iou).mean()  # iou loss

                    # Objectness
                    iou = (iou.detach() * nwd_ratio + nwd.detach() * (1 - nwd_ratio)).clamp(0, 1).type(tobj.dtype)
                else:
                    lbox += (1.0 - iou).mean()  # iou loss

                    # Objectness
                    iou = iou.detach().clamp(0).type(tobj.dtype)
                if self.sort_obj_iou:
                    j = iou.argsort()
                    b, a, gj, gi, iou = b[j], a[j], gj[j], gi[j], iou[j]
                if self.gr < 1:
                    iou = (1.0 - self.gr) + self.gr * iou
                tobj[b, a, gj, gi] = iou  # iou ratio

                # Classification
                if self.nc > 1:  # cls loss (only if multiple classes)
                    t = torch.full_like(pcls, self.cn, device=self.device)  # targets
                    t[range(n), tcls[i]] = self.cp
                    lcls += self.BCEcls(pcls, t)  # BCE

                # Append targets to text file
                # with open('targets.txt', 'a') as file:
                #     [file.write('%11.5g ' * 4 % tuple(x) + '\n') for x in torch.cat((txy[i], twh[i]), 1)]

            obji = self.BCEobj(pi[..., 4], tobj)
            lobj += obji * self.balance[i]  # obj loss
            if self.autobalance:
                self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()

        if self.autobalance:
            self.balance = [x / self.balance[self.ssi] for x in self.balance]
        lbox *= self.hyp['box']
        lobj *= self.hyp['obj']
        lcls *= self.hyp['cls']
        bs = tobj.shape[0]  # batch size

        return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()

    def build_targets(self, p, targets):
        # Build targets for compute_loss(), input targets(image,class,x,y,w,h) targetæ˜¯shapeä¸ºï¼ˆç›®æ ‡æ•°ï¼Œ6ï¼‰çš„tensorï¼Œç¬¬ä¸€åˆ—è¡¨ç¤ºæ ‡ç­¾æ‰€å±çš„å›¾ç‰‡æ ‡ç­¾0~63
        #   è·å–anchorçš„æ•°é‡ å’Œç›®æ ‡çš„æ•°é‡
        na, nt = self.na, targets.shape[0]  # number of anchors, targets
        tcls, tbox, indices, anch = [], [], [], []
        gain = torch.ones(7, device=self.device)  # normalized to gridspace gain
        #   aiçš„shapeä¸ºï¼ˆ3, ntï¼‰ [1, 3] -> [3, 1] -> [3, 63]=[na, nt]
        ai = torch.arange(na, device=self.device).float().view(na, 1).repeat(1, nt)  # same as .repeat_interleave(nt)
        #   åŸæœ¬targets.shape=[nt,6],[image,class,x,y,w,h]ï¼Œrepeatæ ¹æ®æ¯ä¸€å±‚çš„anchoræ•°é‡å°†targetså¢åŠ ä¸€ç»´,shape[na=3,nt,6]
        #   ai[:, :, None] 2ç»´å˜ï¼“ç»´torch.Size([3, nt, 1])
        #   torch.catä¸¤ä¸ª3ç»´çš„tensortåœ¨ç¬¬2ç»´ä¸Šconcat,targetsçš„torch.Size([3, nt, 7])
        #   ç¬¬ä¸€ç»´å¢åŠ layerçš„ç´¢å¼•ï¼Œå¹¶ä¸”å°†åŸæœ¬targets[image,class,x,y,w,h]åœ¨æœ€åä¸Šå¢åŠ anchorçš„ç´¢å¼•[image,class,x,y,w,h,anchor indices],
        #   ä¹Ÿå°±è¯´æŠŠæ¯ä¸ªgtæ¡†åˆ†é…ç»™äº†æ¯ä¸€å±‚è¾“å‡ºçš„æ¯ä¸€ä¸ªanchor [63, 6] [3, 63] -> [3, 63, 6] [3, 63, 1] -> [3, 63, 7]  7: [image_index+class+xywh+anchor_index]
        targets = torch.cat((targets.repeat(na, 1, 1), ai[..., None]), 2)  # append anchor indices

        g = 0.5  # bias ä¸­å¿ƒåç§»ï¼Œç”¨æ¥è¡¡é‡targetç¦»å“ªä¸ªæ ¼å­æ›´è¿‘
        off = torch.tensor(
            [#   ä»¥è‡ªèº«+å‘¨å›´å·¦ä¸Šå³ä¸‹4ä¸ªç½‘æ ¼ = 5ä¸ªç½‘æ ¼ï¼Œç”¨æ¥è®¡ç®—offsets
                [0, 0],
                [1, 0],
                [0, 1],
                [-1, 0],
                [0, -1],  # j,k,l,m
                # [1, 1], [1, -1], [-1, 1], [-1, -1],  # jk,jm,lk,lm
            ],
            device=self.device).float() * g  # offsets

        for i in range(self.nl):
            #   anchorsï¼š3x2
            #   shape:bs, 3, 80, 80, 85
            anchors, shape = self.anchors[i], p[i].shape
            #   è¯»å‡ºç½‘ç»œç¬¬iä¸ªè¾“å‡ºçš„ç‰¹å¾å›¾å¤§å°
            gain[2:6] = torch.tensor(shape)[[3, 2, 3, 2]]  # xyxy gain

            # Match targets to anchors
            #   å°†æ ‡ç­¾åŒ¹é…åˆ°ç‰¹å¾å›¾å¤§å°
            t = targets * gain  # shape(3,n,7)
            if nt:
                # Matches
                #   yolov5æŠ›å¼ƒäº†MaxiouåŒ¹é…è§„åˆ™ï¼Œè®¡ç®—æ ‡ç­¾boxå’Œå½“å‰å±‚anchorsçš„å®½é«˜æ¯”ï¼Œwb/wa,hb/ha
                #   å¦‚æœå®½é«˜æ¯”å¤§äºè®¾å®šçš„é˜ˆå€¼è¯´æ˜è¯¥boxåœ¨è¯¥å±‚æ²¡æœ‰åˆé€‚çš„anchorï¼Œåœ¨è¯¥é¢„æµ‹å±‚ä¹‹é—´å°†è¿™äº›boxå½“èƒŒæ™¯
                #   å¯¹bä¸­ä¿å­˜ä¸‹æ¥çš„gtè¿›è¡Œæ‰©å……
                #   1ï¼‰ä¿å­˜boxä¸­å¿ƒç‚¹åæ ‡Xcè·ç¦»ç½‘æ ¼å·¦è¾¹çš„è·ç¦»å°äº0.5ä¸”åæ ‡å¤§äº1çš„box
                #   2ï¼‰ä¿å­˜boxä¸­å¿ƒç‚¹åæ ‡Ycè·ç¦»ç½‘æ ¼ä¸Šè¾¹è·ç¦»å°äº0.5ä¸”åæ ‡å¤§äº1çš„box
                #   3ï¼‰ä¿å­˜boxä¸­å¿ƒç‚¹åæ ‡Xcè·ç¦»ç½‘æ ¼å³è¾¹è·ç¦»å°äº0.5ä¸”åæ ‡å¤§äº1çš„box
                #   4ï¼‰ä¿å­˜boxä¸­å¿ƒç‚¹åæ ‡Ycè·ç¦»ç½‘æ ¼ä¸‹ä¸Šè¾¹è·ç¦»å°äº0.5ä¸”åæ ‡å¤§äº1çš„box
                #   è¡¥å……:ä¸ºä»€ä¹ˆä¼šå–è·ç¦»å››è¾¹å°äº0.5çš„ç‚¹,æ˜¯å› ä¸ºç­‰äº0.5æ—¶,æˆ‘ä»¬è®¤ä¸ºè¯¥boxæ­£å¥½è½åˆ°è¯¥ç½‘æ ¼ä¸­,
                #   ä½†æ˜¯å°äº0.5æ—¶,å¯èƒ½æ˜¯å› ä¸ºåœ¨ç½‘ç»œä¸æ–­é™é‡‡æ ·æ—¶,å¯¹ç‰¹å¾å›¾å°ºåº¦è¿›è¡Œå–æ•´å¯¼è‡´boxä¸­å¿ƒäº§ç”Ÿäº†åå·®,
                #   æ‰€ä»¥ä½œè€…å°†å°äº0.5çš„boxå‡å»åæ‰§1ï¼ˆoffçŸ©é˜µï¼‰,ä½¿å¾—boxä¸­å¿ƒç§»åŠ¨åˆ°ç›¸é‚»çš„ç‰¹å¾å›¾ç½‘æ ¼ä¸­,
                #   ä»è€Œå¯¹æ­£æ ·æœ¬è¿›è¡Œæ‰©å……,ä¿è¯äº†åå·®å¯¼è‡´çš„boxé”™ä½ä»¥åŠæ‰©å……äº†æ­£æ ·æœ¬çš„ä¸ªæ•°
                #   anchors[:, None]å°†[3, 2]æ‰©å……ä¸ºäº†[3, 1, 2]
                #   rçš„å½¢çŠ¶[3, nt, 2] è¡¨ç¤ºæ ‡ç­¾å’Œanchorçš„æ¯”å€¼
                r = t[..., 4:6] / anchors[:, None]  # wh ratio
                #   jçš„å½¢çŠ¶[3, nt]
                # ç­›é€‰æ¡ä»¶  GTä¸anchorçš„å®½æ¯”æˆ–é«˜æ¯”è¶…è¿‡ä¸€å®šçš„é˜ˆå€¼ å°±å½“ä½œè´Ÿæ ·æœ¬
                # torch.max(r, 1. / r)=[3, 63, 2] ç­›é€‰å‡ºå®½æ¯”w1/w2 w2/w1 é«˜æ¯”h1/h2 h2/h1ä¸­æœ€å¤§çš„é‚£ä¸ª
                # .max(2)è¿”å›å®½æ¯” é«˜æ¯”ä¸¤è€…ä¸­è¾ƒå¤§çš„ä¸€ä¸ªå€¼å’Œå®ƒçš„ç´¢å¼•  [0]è¿”å›è¾ƒå¤§çš„ä¸€ä¸ªå€¼
                # j: [3, 63]  False: å½“å‰anchoræ˜¯å½“å‰gtçš„è´Ÿæ ·æœ¬  True: å½“å‰anchoræ˜¯å½“å‰gtçš„æ­£æ ·æœ¬
                j = torch.max(r, 1 / r).max(2)[0] < self.hyp['anchor_t']  # compare
                # j = wh_iou(anchors, t[:, 4:6]) > model.hyp['iou_t']  # iou(3,n)=wh_iou(anchors(3,2), gwh(n,2))
                # t: [3, 63, 7] -> [126, 7]  [num_Positive_sample, image_index+class+xywh+anchor_index]
                t = t[j]  # filter

                # Offsets
                #   è·å¾—boxçš„ä¸­å¿ƒç‚¹åæ ‡
                gxy = t[:, 2:4]  # grid xy
                #   å¹¶è½¬æ¢ä¸ºä»¥ç‰¹å¾å›¾å³ä¸‹è§’ä¸ºåŸç‚¹çš„åæ ‡
                gxi = gain[[2, 3]] - gxy  # inverse
                # ç­›é€‰ä¸­å¿ƒåæ ‡ è·ç¦»å½“å‰grid_cellçš„å·¦ã€ä¸Šæ–¹åç§»å°äºg=0.5 ä¸” ä¸­å¿ƒåæ ‡å¿…é¡»å¤§äº1(åæ ‡ä¸èƒ½åœ¨è¾¹ä¸Š æ­¤æ—¶å°±æ²¡æœ‰4ä¸ªæ ¼å­äº†)
                # j: [126] bool å¦‚æœæ˜¯Trueè¡¨ç¤ºå½“å‰targetä¸­å¿ƒç‚¹æ‰€åœ¨çš„æ ¼å­çš„å·¦è¾¹æ ¼å­ä¹Ÿå¯¹è¯¥targetè¿›è¡Œå›å½’(åç»­è¿›è¡Œè®¡ç®—æŸå¤±)
                # k: [126] bool å¦‚æœæ˜¯Trueè¡¨ç¤ºå½“å‰targetä¸­å¿ƒç‚¹æ‰€åœ¨çš„æ ¼å­çš„ä¸Šè¾¹æ ¼å­ä¹Ÿå¯¹è¯¥targetè¿›è¡Œå›å½’(åç»­è¿›è¡Œè®¡ç®—æŸå¤±)
                j, k = ((gxy % 1 < g) & (gxy > 1)).T
                # ç­›é€‰ä¸­å¿ƒåæ ‡ è·ç¦»å½“å‰grid_cellçš„å³ã€ä¸‹æ–¹åç§»å°äºg=0.5 ä¸” ä¸­å¿ƒåæ ‡å¿…é¡»å¤§äº1(åæ ‡ä¸èƒ½åœ¨è¾¹ä¸Š æ­¤æ—¶å°±æ²¡æœ‰4ä¸ªæ ¼å­äº†)
                # l: [126] bool å¦‚æœæ˜¯Trueè¡¨ç¤ºå½“å‰targetä¸­å¿ƒç‚¹æ‰€åœ¨çš„æ ¼å­çš„å³è¾¹æ ¼å­ä¹Ÿå¯¹è¯¥targetè¿›è¡Œå›å½’(åç»­è¿›è¡Œè®¡ç®—æŸå¤±)
                # m: [126] bool å¦‚æœæ˜¯Trueè¡¨ç¤ºå½“å‰targetä¸­å¿ƒç‚¹æ‰€åœ¨çš„æ ¼å­çš„ä¸‹è¾¹æ ¼å­ä¹Ÿå¯¹è¯¥targetè¿›è¡Œå›å½’(åç»­è¿›è¡Œè®¡ç®—æŸå¤±)
                l, m = ((gxi % 1 < g) & (gxi > 1)).T
                # j: [5, 126]  torch.ones_like(j): å½“å‰æ ¼å­, ä¸éœ€è¦ç­›é€‰å…¨æ˜¯True  j, k, l, m: å·¦ä¸Šå³ä¸‹æ ¼å­çš„ç­›é€‰ç»“æœ
                j = torch.stack((torch.ones_like(j), j, k, l, m))
                # å¾—åˆ°ç­›é€‰åæ‰€æœ‰æ ¼å­çš„æ­£æ ·æœ¬ æ ¼å­æ•°<=3*126 éƒ½ä¸åœ¨è¾¹ä¸Šç­‰å·æˆç«‹
                # t: [126, 7] -> å¤åˆ¶5ä»½target[5, 126, 7]  åˆ†åˆ«å¯¹åº”å½“å‰æ ¼å­å’Œå·¦ä¸Šå³ä¸‹æ ¼å­5ä¸ªæ ¼å­
                # j: [5, 126] + t: [5, 126, 7] => t: [378, 7] ç†è®ºä¸Šæ˜¯å°äºç­‰äº3å€çš„126 å½“ä¸”ä»…å½“æ²¡æœ‰è¾¹ç•Œçš„æ ¼å­ç­‰å·æˆç«‹
                t = t.repeat((5, 1, 1))[j]
                # torch.zeros_like(gxy)[None]: [1, 126, 2]   off[:, None]: [5, 1, 2]  => [5, 126, 2]
                # jç­›é€‰å: [378, 2]  å¾—åˆ°æ‰€æœ‰ç­›é€‰åçš„ç½‘æ ¼çš„ä¸­å¿ƒç›¸å¯¹äºè¿™ä¸ªè¦é¢„æµ‹çš„çœŸå®æ¡†æ‰€åœ¨ç½‘æ ¼è¾¹ç•Œï¼ˆå·¦å³ä¸Šä¸‹è¾¹æ¡†ï¼‰çš„åç§»é‡
                offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]
            else:
                t = targets[0]
                offsets = 0

            # Define å°†tæ²¿1è½´åˆ†ä¸º4ä»½ï¼Œ7/4é™¤ä¸å°½ï¼Œåˆ†ä¸º2ï¼Œ2ï¼Œ2ï¼Œ1
            bc, gxy, gwh, a = t.chunk(4, 1)  # (image, class), grid xy, grid wh, anchors
            a, (b, c) = a.long().view(-1), bc.long().T  # anchors, image, class
            # é¢„æµ‹çœŸå®æ¡†çš„ç½‘æ ¼æ‰€åœ¨çš„å·¦ä¸Šè§’åæ ‡(æœ‰å·¦ä¸Šå³ä¸‹çš„ç½‘æ ¼)
            gij = (gxy - offsets).long()
            gi, gj = gij.T  # grid indices

            # Append
            indices.append((b, a, gj.clamp_(0, shape[2] - 1), gi.clamp_(0, shape[3] - 1)))  # image, anchor, grid
            tbox.append(torch.cat((gxy - gij, gwh), 1))  # box
            anch.append(anchors[a])  # anchors
            tcls.append(c)  # class

        return tcls, tbox, indices, anch




#-----------------------#
#   TOOD loss
#-----------------------#

class ComputeTLoss:
    def __init__(self, model):
        device = next(model.parameters()).device
        h = model.hyp # è¶…å‚æ•°
        m = de_parallel(model).model[-1] # Detect_T
        self.bce = nn.BCEWithLogitsLoss(reduction='none')
        self.na = m.na
        self.nc = m.nc
        self.nl = m.nl
        self.no = m.no # self.na * (num_classes + 4 * reg_max)
        self.hyp = h
        self.stride = m.stride
        self.device = device
        self.reg_max = m.reg_max
        self.use_dfl = m.reg_max > 1
        self.assigner = TaskAlignedAssigner(topk=10, num_classes=self.nc, alpha=0.5, beta=6.0)
        self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)
        self.bbox_loss = BboxLoss(m.reg_max - 1, use_dfl=self.use_dfl).to(device)
        # self.anchors = m.anchors

    def preprocess(self, targets, batch_size, scale_tensor):
        if targets.shape[0] == 0:
            out = torch.zeros(batch_size, 0, 5, device=self.device)
        else:
            i = targets[:, 0]  # image index
            _, counts = i.unique(return_counts=True)
            out = torch.zeros(batch_size, counts.max(), 5, device=self.device)
            for j in range(batch_size):
                matches = i == j
                n = matches.sum()
                if n:
                    out[j, :n] = targets[matches, 1:]
            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))
        return out

    def bbox_decode(self, anchor_points, pred_dist):
        if self.use_dfl:
            b, a, c = pred_dist.shape  # batch, anchors, channels
            pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))
            # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))
            # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)
        return dist2bbox(pred_dist, anchor_points, xywh=False)

    def __call__(self, preds, targets):
        loss = torch.zeros(3, device=self.device)  # box, cls, dfl
        feats = preds[1] if isinstance(preds, tuple) else preds
        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(
            (self.reg_max * 4, self.nc), 1)

        pred_scores = pred_scores.permute(0, 2, 1).contiguous()
        pred_distri = pred_distri.permute(0, 2, 1).contiguous()

        dtype = pred_scores.dtype
        batch_size = pred_scores.shape[0]
        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)
        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)

        # targets
        # targets = torch.cat((batch["batch_idx"].view(-1, 1), batch["cls"].view(-1, 1), batch["bboxes"]), 1)
        targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])
        gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy
        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)

        # pboxes
        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)

        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(
            pred_scores.detach().sigmoid(), (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),
            anchor_points * stride_tensor, gt_labels, gt_bboxes, mask_gt)

        target_bboxes /= stride_tensor
        target_scores_sum = max(target_scores.sum(), 1)

        # cls loss
        loss[2] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum

        # bbox loss
        if fg_mask.sum():
            loss[0], loss[1] = self.bbox_loss(pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask)

        loss[2] *= 0.5
        loss[0] *= 7.5
        loss[1] *= 1.5

        return loss.sum() * batch_size, loss.detach()


#-----------------------#
#   distill loss
#-----------------------#
def compute_distillation_output_loss(p, t_p, model, d_weight=1):
    t_ft = torch.cuda.FloatTensor if t_p[0].is_cuda else torch.Tensor
    t_lcls, t_lbox, t_lobj = t_ft([0]), t_ft([0]), t_ft([0])
    h = model.hyp  # hyperparameters
    red = 'mean'  # Loss reduction (sum or mean)
    if red != "mean":
        raise NotImplementedError("reduction must be mean in distillation mode!")

    DboxLoss = nn.MSELoss(reduction="none")
    DclsLoss = nn.MSELoss(reduction="none")
    DobjLoss = nn.MSELoss(reduction="none")
    # per output
    for i, pi in enumerate(p):  # layer index, layer predictions
        t_pi = t_p[i]
        t_obj_scale = t_pi[..., 4].sigmoid()

        # BBox
        b_obj_scale = t_obj_scale.unsqueeze(-1).repeat(1, 1, 1, 1, 4)
        t_lbox += torch.mean(DboxLoss(pi[..., :4], t_pi[..., :4]) * b_obj_scale)

        # Class
        if model.nc > 1:  # cls loss (only if multiple classes)
            c_obj_scale = t_obj_scale.unsqueeze(-1).repeat(1, 1, 1, 1, model.nc)
            # t_lcls += torch.mean(c_obj_scale * (pi[..., 5:] - t_pi[..., 5:]) ** 2)
            t_lcls += torch.mean(DclsLoss(pi[..., 5:], t_pi[..., 5:]) * c_obj_scale)

        # t_lobj += torch.mean(t_obj_scale * (pi[..., 4] - t_pi[..., 4]) ** 2)
        t_lobj += torch.mean(DobjLoss(pi[..., 4], t_pi[..., 4]) * t_obj_scale)
    t_lbox *= h['box']
    t_lobj *= h['obj']
    t_lcls *= h['cls']
    # bs = p[0].shape[0]  # batch size
    loss = (t_lobj + t_lbox + t_lcls) * d_weight
    return loss
